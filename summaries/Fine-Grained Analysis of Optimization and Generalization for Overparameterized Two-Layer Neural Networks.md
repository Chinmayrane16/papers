##### [19-02-22] [paper45]
- Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks [[pdf]](https://arxiv.org/abs/1901.08584) [[pdf with comments]](https://github.com/fregu856/papers/blob/master/commented_pdfs/Fine-Grained%20Analysis%20of%20Optimization%20and%20Generalization%20for%20Overparameterized%20Two-Layer%20Neural%20Networks.pdf)
- *Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruosong Wang*
- `2019-01-24`

****

### General comments on paper quality:
- Somewhat interesting paper that is quite theoretical. I found it to be a rather heavy read, and I did not fully understand absolutely everything.

### Comments:
- I did not quite get why they fix the weights a_i of the second layer?

- They use gradient descent (GD) instead of SGD, could you obtain similar results also for SGD?

- I think that I probably did not understand the paper well enough to really be able to judge how significant/interesting the presented results actually are. How restrictive are their assumptions? In what way / to what extent could these results be of practical use in real-world applications?

- The reference section seems like a pretty neat resource for previous work on characterization of NN loss landscapes etc. 
