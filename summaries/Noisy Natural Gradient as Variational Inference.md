##### [19-02-07] [paper40]
- Noisy Natural Gradient as Variational Inference [[pdf]](https://arxiv.org/abs/1712.02390) [[video]](https://youtu.be/bWItvHYqKl8) [[code]](https://github.com/pomonam/NoisyNaturalGradient) [[pdf with comments]](https://github.com/fregu856/papers/blob/master/commented_pdfs/Noisy%20Natural%20Gradient%20as%20Variational%20Inference.pdf)
- *Guodong Zhang, Shengyang Sun, David Duvenaud, Roger Grosse*
- `2017-12-06, ICML2018`

****

### General comments on paper quality:
- Well-written and somewhat interesting paper.

### Comments:
- Quite a heavy read for me as I am not particularly familiar with natural gradient optimization methods or K-FAC.

- I get that not being restricted to just fully-factorized Gaussian variational posteriors is something that could improve performance, but is it actually practical for properly large networks?

- They mention previous work on extending variational methods to non-fully-factorized posteriors, but I found them quite difficult to compare. It is not clear to me whether or not the presented method actually is a clear improvement (either in terms of performance or practicality).
